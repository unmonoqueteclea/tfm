<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <meta http-equiv="Content-Style-Type" content="text/css" />
        <meta name="generator" content="pandoc" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
                        <title>01_title_page</title>
        <style type="text/css">code{white-space: pre;}</style>
                                                    <style>
            body {
                font-family: Georgia;
                max-width: 800px;
                margin: 0 auto;
                line-height: 30px;
                font-size: 18px;
                padding-left: 350px;
                padding-right: 50px;
                color: #111;
            }
            
            h1, h2, h3, h4, h5, h6 {
                font-family: Arial;
            }
            
            h1 {
                padding-top: 200px;
                line-height: 50px;
            }
            h2 {
                padding-top: 30px;
            }
            h3 {
                padding-top: 20px;
            }
            h4 {
                padding-top: 10px;
            }
            p {
                text-align: justify;
            }
            p a {
                word-wrap: break-word;
                white-space: pre;
            }
            code {
                word-wrap: break-word;
            }
            blockquote {
                border-left: 3px solid #eee;
                margin-left: 20px;
                padding-left: 20px;
            }
            
            ::selection {
                background-color: #E4E4E4;
            }
            
            table {
                width: 100%;
            }
            table caption {
                font-weight: bold;
            }
            table tr {
                padding: 0;
                margin: 0;
                background-color: #f0f0f0;
            }
            table tr.even {
                background-color: #fafafa;
            }
            table td {
                margin: 0;
                padding: 3px 5px;
            }
            
            p span.added {
                color: green;
                background-color: #FFF3C5;
            }
            p span.removed {
                color: red;
                background-color: #FFF3C5;
            }
            
            #title-page {
                padding: 80px 0;
            }
            
            #TOC {
                position: fixed;
                left: 0;
                top: 0;
                overflow-y: scroll;
                height: 100%;
                background: #fafafa;
                max-width: 300px;
                font-family: Arial;
                font-size: 15px;
                line-height: 30px;
            }
            ::-webkit-scrollbar {
                width: 8px;
            }
            ::-webkit-scrollbar-track {
                background-color: #ECECEC;
            }
            ::-webkit-scrollbar-thumb {
                background-color: #B0B0B0;
                border-radius: 8px;
            }
            #TOC > ul {
                padding-right: 10px;
            }
            #TOC ul {
                list-style: none;
                padding-left: 20px;
            }
            #TOC ul li a {
                text-decoration: none;
                color: #364149;
                text-overflow: ellipsis;
                display: block;
                white-space: nowrap;
                overflow: hidden;
            }
            #TOC ul li a:hover {
                color: #008cff;
            }
            
            .figure {
                text-align: center;
            }
            .figure p {
                text-align: center;
                font-style: italic;
            }
            .figure img {
                  width: 100%;
            }
            </style>
                <script src="js/jquery.js"></script>
        <script src="js/diff.js"></script>
        <script src="js/main.js"></script>
    </head>
    <body>
                <!--
                -->
        <div id="title-page">
            <h1>This is the title of the thesis</h1>
            <h2>Firstname Surname</h2>
        </div>
                    <div id="TOC">
                <ul>
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#acknowledgements">Acknowledgements</a></li>
                <li><a href="#list-of-figures">List of figures</a></li>
                <li><a href="#list-of-tables">List of tables</a></li>
                <li><a href="#abbreviations">Abbreviations</a></li>
                <li><a href="#introducción"><span class="toc-section-number">1</span> Introducción</a><ul>
                <li><a href="#motivación"><span class="toc-section-number">1.1</span> Motivación</a></li>
                <li><a href="#objectivos"><span class="toc-section-number">1.2</span> Objectivos</a></li>
                <li><a href="#principales-contribuciones"><span class="toc-section-number">1.3</span> Principales contribuciones</a></li>
                <li><a href="#estructura"><span class="toc-section-number">1.4</span> Estructura</a></li>
                </ul></li>
                <li><a href="#el-ojo-y-sus-patologías"><span class="toc-section-number">2</span> El ojo y sus patologías</a><ul>
                <li><a href="#anatomía-y-fisiología-ocular"><span class="toc-section-number">2.1</span> Anatomía y fisiología ocular</a><ul>
                <li><a href="#la-retina-y-su-importancia"><span class="toc-section-number">2.1.1</span> La retina y su importancia</a></li>
                </ul></li>
                <li><a href="#principales-patologías-de-la-retina"><span class="toc-section-number">2.2</span> Principales patologías de la retina</a><ul>
                <li><a href="#retinopatía-diabética"><span class="toc-section-number">2.2.1</span> Retinopatía diabética</a></li>
                <li><a href="#degeneración-macular-asociada-a-la-edad"><span class="toc-section-number">2.2.2</span> Degeneración macular asociada a la edad</a></li>
                <li><a href="#sistemas-de-diagnóstico"><span class="toc-section-number">2.2.3</span> Sistemas de diagnóstico</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#machine-learning-y-aplicaciones-médicas"><span class="toc-section-number">3</span> Machine Learning y aplicaciones médicas</a><ul>
                <li><a href="#ia-big-data-machine-learning-y-deep-learning"><span class="toc-section-number">3.1</span> IA, Big Data, Machine Learning y Deep Learning</a></li>
                <li><a href="#redes-neuronales-descenso-de-gradiente-y-backpropagation"><span class="toc-section-number">3.2</span> Redes neuronales, descenso de gradiente y backpropagation</a><ul>
                <li><a href="#redes-neuronales-convolucionales"><span class="toc-section-number">3.2.1</span> Redes neuronales convolucionales</a></li>
                <li><a href="#arquitecturas-de-redes-neuronales"><span class="toc-section-number">3.2.2</span> Arquitecturas de redes neuronales</a></li>
                </ul></li>
                <li><a href="#medidas-de-evaluación-de-sistemas-de-deep-learning"><span class="toc-section-number">3.3</span> Medidas de evaluación de sistemas de Deep Learning</a></li>
                <li><a href="#transfer-learning"><span class="toc-section-number">3.4</span> Transfer Learning</a><ul>
                <li><a href="#transfer-learning-con-imágenes"><span class="toc-section-number">3.4.1</span> Transfer Learning con imágenes</a></li>
                </ul></li>
                <li><a href="#aplicaciones-médicas-del-deep-learning"><span class="toc-section-number">3.5</span> Aplicaciones médicas del Deep Learning</a><ul>
                <li><a href="#prognosis"><span class="toc-section-number">3.5.1</span> Prognosis</a></li>
                <li><a href="#diagnosis"><span class="toc-section-number">3.5.2</span> Diagnosis</a></li>
                <li><a href="#tratamiento"><span class="toc-section-number">3.5.3</span> Tratamiento</a></li>
                <li><a href="#clinician-workflow"><span class="toc-section-number">3.5.4</span> Clinician workflow</a></li>
                <li><a href="#correlación-no-implica-causalidad"><span class="toc-section-number">3.5.5</span> Correlación no implica causalidad</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#estado-del-arte"><span class="toc-section-number">4</span> Estado del arte</a><ul>
                <li><a href="#detección-de-dr"><span class="toc-section-number">4.1</span> Detección de DR</a></li>
                <li><a href="#detección-de-dmae"><span class="toc-section-number">4.2</span> Detección de DMAE</a></li>
                <li><a href="#introduction"><span class="toc-section-number">4.3</span> Introduction</a></li>
                <li><a href="#method"><span class="toc-section-number">4.4</span> Method</a><ul>
                <li><a href="#subsection-1"><span class="toc-section-number">4.4.1</span> Subsection 1</a></li>
                <li><a href="#subsection-2"><span class="toc-section-number">4.4.2</span> Subsection 2</a></li>
                </ul></li>
                <li><a href="#results"><span class="toc-section-number">4.5</span> Results</a></li>
                <li><a href="#discussion"><span class="toc-section-number">4.6</span> Discussion</a></li>
                <li><a href="#conclusion"><span class="toc-section-number">4.7</span> Conclusion</a></li>
                </ul></li>
                <li><a href="#diseño-de-sistema-para-la-clasificación-automática-de-rd-y-dmae"><span class="toc-section-number">5</span> Diseño de sistema para la clasificación automática de RD y DMAE</a><ul>
                <li><a href="#exploración-inicial-de-los-datos"><span class="toc-section-number">5.1</span> Exploración inicial de los datos</a></li>
                <li><a href="#introduction-1"><span class="toc-section-number">5.2</span> Introduction</a></li>
                <li><a href="#method-1"><span class="toc-section-number">5.3</span> Method</a><ul>
                <li><a href="#subsection-1-1"><span class="toc-section-number">5.3.1</span> Subsection 1</a></li>
                <li><a href="#subsection-2-1"><span class="toc-section-number">5.3.2</span> Subsection 2</a></li>
                </ul></li>
                <li><a href="#results-1"><span class="toc-section-number">5.4</span> Results</a></li>
                <li><a href="#discussion-1"><span class="toc-section-number">5.5</span> Discussion</a></li>
                <li><a href="#conclusion-1"><span class="toc-section-number">5.6</span> Conclusion</a></li>
                </ul></li>
                <li><a href="#resultados-obtenidos"><span class="toc-section-number">6</span> Resultados obtenidos</a><ul>
                <li><a href="#introduction-2"><span class="toc-section-number">6.1</span> Introduction</a></li>
                <li><a href="#method-2"><span class="toc-section-number">6.2</span> Method</a><ul>
                <li><a href="#subsection-1-2"><span class="toc-section-number">6.2.1</span> Subsection 1</a></li>
                <li><a href="#subsection-2-2"><span class="toc-section-number">6.2.2</span> Subsection 2</a></li>
                </ul></li>
                <li><a href="#results-2"><span class="toc-section-number">6.3</span> Results</a></li>
                <li><a href="#discussion-2"><span class="toc-section-number">6.4</span> Discussion</a></li>
                <li><a href="#conclusion-2"><span class="toc-section-number">6.5</span> Conclusion</a></li>
                </ul></li>
                <li><a href="#conclusiones"><span class="toc-section-number">7</span> Conclusiones</a><ul>
                <li><a href="#thesis-summary"><span class="toc-section-number">7.1</span> Thesis summary</a></li>
                <li><a href="#future-work"><span class="toc-section-number">7.2</span> Future work</a></li>
                </ul></li>
                <li><a href="#appendix-1-some-extra-stuff">Appendix 1: Some extra stuff</a></li>
                <li><a href="#appendix-2-some-more-extra-stuff">Appendix 2: Some more extra stuff</a></li>
                <li><a href="#references">References</a></li>
                </ul>
            </div>
                                <!-- 
This is the Latex-heavy title page. 
People outside UCL may want to remove the header logo 
and add the centred logo
-->

<!-- This page is for an official declaration. -->

<p>   </p>
<h1 id="abstract" class="unnumbered">Abstract</h1>
<!-- This is the abstract -->
<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam et turpis gravida, lacinia ante sit amet, sollicitudin erat. Aliquam efficitur vehicula leo sed condimentum. Phasellus lobortis eros vitae rutrum egestas. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Donec at urna imperdiet, vulputate orci eu, sollicitudin leo. Donec nec dui sagittis, malesuada erat eget, vulputate tellus. Nam ullamcorper efficitur iaculis. Mauris eu vehicula nibh. In lectus turpis, tempor at felis a, egestas fermentum massa.</p>

<h1 id="acknowledgements" class="unnumbered">Acknowledgements</h1>
<!-- This is for acknowledging all of the people who helped out -->
<p>Interdum et malesuada fames ac ante ipsum primis in faucibus. Aliquam congue fermentum ante, semper porta nisl consectetur ut. Duis ornare sit amet dui ac faucibus. Phasellus ullamcorper leo vitae arcu ultricies cursus. Duis tristique lacus eget metus bibendum, at dapibus ante malesuada. In dictum nulla nec porta varius. Fusce et elit eget sapien fringilla maximus in sit amet dui.</p>
<p>Mauris eget blandit nisi, faucibus imperdiet odio. Suspendisse blandit dolor sed tellus venenatis, venenatis fringilla turpis pretium. Donec pharetra arcu vitae euismod tincidunt. Morbi ut turpis volutpat, ultrices felis non, finibus justo. Proin convallis accumsan sem ac vulputate. Sed rhoncus ipsum eu urna placerat, sed rhoncus erat facilisis. Praesent vitae vestibulum dui. Proin interdum tellus ac velit varius, sed finibus turpis placerat.</p>
<!-- Use the \newpage command to force a new page -->




<h1 id="list-of-figures" class="unnumbered">List of figures</h1>
<!--
For me, this was the only drawback of writing in Markdown: it is not possible to add a short caption to figures and tables. This means that the \listoftables and \listoffigures commands will generate lists using the full titles, which is probably isn't what you want. For now, the solution is to create the lists manually, when everything else is finished.
-->
<p>Figure 4.1 This is an example figure . . . <br />
Figure x.x Short title of the figure . . . </p>


<h1 id="list-of-tables" class="unnumbered">List of tables</h1>
<!-- 
For me, this was the only drawback of writing in Markdown: it is not possible to add a short caption to figures and tables. This means that the \listoftables and \listoffigures commands will generate lists using the full titles, which is probably isn't what you want. For now, the solution is to create the lists manually, when everything else is finished.
-->
<p>Table 5.1 This is an example table . . . <br />
Table x.x Short title of the figure . . . </p>
<h1 id="abbreviations" class="unnumbered">Abbreviations</h1>


<!--
Para crear PDF: make pdf
For italic, add one * on either side of the text
For bold, add two * on either side of the text
For bold and italic, add _** on either side of the text
Ejemplo cita: [@Cousteau1963]
Latex: (@ref_for_eqn1) $f(x) = ax^3 + bx^2 + cx + d$
Lista desordenada:
    - item
    - item

For syntax highlighting in code blocks, add three "`"
characters before and after a code block:

```python
mood = 'happy'
if mood == 'happy':
    print("I am a happy robot")
```

Alternatively, you can also use LaTeX to create a code block as shown
in the Java example below: \lstinputlisting[style=javaCodeStyle,
caption=Main.java]{source/code/HelloWorld.java}

If you use `javaCodeStyle` as defined in the `preamble.tex`, it is
best to keep the maximum line length in the source code at 80
characters.

-->
<h1 id="introducción"><span class="header-section-number">1</span> Introducción</h1>
<!-- Comienzo:
     Previsión:
     Fin V1:
     Papers útiles:
-->
<h2 id="motivación"><span class="header-section-number">1.1</span> Motivación</h2>
<!-- Comienzo:
     Previsión:
     Fin V1:
     Papers útiles:
-->
<h2 id="objectivos"><span class="header-section-number">1.2</span> Objectivos</h2>
<!-- Comienzo:
     Previsión:
     Fin V1:
     Papers útiles:
-->
<h2 id="principales-contribuciones"><span class="header-section-number">1.3</span> Principales contribuciones</h2>
<!-- Comienzo:
     Previsión:
     Fin V1:
     Papers útiles:
-->
<h2 id="estructura"><span class="header-section-number">1.4</span> Estructura</h2>
<!-- Comienzo:
     Previsión:
     Fin V1:
     Papers útiles:
-->
<h1 id="el-ojo-y-sus-patologías"><span class="header-section-number">2</span> El ojo y sus patologías</h1>
<!-- Comienzo: 14 Mayo
     Previsión: Miercoles 22 Mayo
     Papers útiles: (P0) (P7) (P8)
-->
<p>Aunque comúnmente se suele hablar de los ojos como de los ojos como nuestra ventana al exterior <span class="citation" data-cites="zhu2001eye">(Zhu et al. 2001)</span>, la realidad es que su funcionamiento y estructura es considerablemente más complicado que el de una simple ventana de cristal. Dada su extrema perfección, Charles Darwin reconoció tener grandes dificultades para explicar los ojos únicamente mediante variación y selección.<span class="citation" data-cites="darwin2004origin">(Darwin 2004)</span></p>
<h2 id="anatomía-y-fisiología-ocular"><span class="header-section-number">2.1</span> Anatomía y fisiología ocular</h2>
<!-- Comienzo: 14 Mayo
     Fin V1: 15 Mayo
     Papers útiles: (P0) (P7) (P8)
-->
<p>Los ojos son el principal órgano de la visión. La perfección del ojo es tal, que cada ojo ha evolucionado adaptándose a las necesidades del organismo poseedor, esto ha provocado que existan diversas diferencias en la anatomía y fisiología ocular de los diferentes organismos. <span class="citation" data-cites="zhu2001eye">(Zhu et al. 2001)</span>.</p>
<p>La estructura más simple de ojo consiste en una concentración de células fotorreceptoras mediante las cuales un organismo puede distinguir, no sólo la luz y la oscuridad, sino la dirección de la luz incidente. Esta última característica supondría para los organismos con este tipo de sistema ocular una ventaja evolutiva ante otros tipos de organismos que únicamente podrían diferenciar entre luz y oscuridad.</p>
<p>Sin embargo, el sistema óptico complejo presente en el 96% de las especies animales, es capaz de realizar un proceso completo que comienza con la detección de la luz y termina con unos impulsos electroquímicos viajando a través de las neuronas. Durante ese proceso, los ojos tienen que captar la luz, regular la intensidad mediante un diafragma y, mediante un sistema de lentes (cristalina) enfocarla en único punto que se encargará de realizar la transformación en impulsos eléctricos. Este punto, que será objeto de estudio durante este trabajo, es conocido como la retina.</p>
<!-- TODO: Imagen del globo ocular con sus partes -->
<p>La anatomía y fisiología ocular es similar en la mayoría de los vertebrados. El globo ocular, que contiene el resto de elementos del sistema, es una esfera llena de <strong>humor acuoso</strong>, que es un líquido compuesto en un 99% por agua. El constante flujo de este líquido en el ojo permite regular la presión ocular, de forma que las propiedades del ojo puedan mantenerse constantes. Además, también permite aportar nutrientes y oxígeno a la parte anterior del ojo y eliminar deshechos de la parte anterior del ojo, a la que los capilares no son capaces de llegar <span class="citation" data-cites="zhu2001eye">(Zhu et al. 2001)</span>. La pared del globo ocular la forman 3 capas conocidas como (desde la más interna a la más externa): retina, coroides y esclerótica.</p>
<p>Cuando la luz llega al ojo, el primer elemento con el que tiene contacto es la <strong>córnea</strong>, que pertenece a la capa esclerótica. Esta se encargo de refractar la luz, haciéndola converger dentro del ojo. La cornea protege al resto del ojo de polvo, gérmenesy cualquier tipo de sustancia dañina. Además, también filtra los rayos ultravioleta procedentes de la luz solar <span class="citation" data-cites="zhu2001eye">(Zhu et al. 2001)</span>. Debido a su índice de refracción (mayor que el del aire), la córnea también provocará que se desvíen los rayos de luz que lleguen a ella permitiendo así que convergan en el centro del ojo. La mínima dispersión que se producen en los rayos de luz, lo que nos permite obtener una imagen clara y definida, está asegurada por la uniformidad espacial de sus células <span class="citation" data-cites="oyster1999human">(Oyster 1999)</span>.</p>
<p>Posteriormente, es el <strong>iris</strong> quien se encargará de contraer o expandir la <strong>pupila</strong>, lo que permitirá regular la cantidad de luz que entra al ojo. Esta es la razón por la que, en condiciones de baja luminosidad, nuestras pupilas se ven dilatadas, para poder permitir el paso de la mayor cantidad posible de luz.</p>
<p>Posteriormente, y como en otros sistemas ópticos artificiales, necesitamos un elemento que enfoque toda esa luz en un único punto. Este proceso se realiza mediante el <strong>cristalino</strong> que actúa como lente, y una serie de músculos a su alrededor que modifican su forma (y su índice de refracción) para permitirnos enfocar objetos a diferentes distancias. Al igual que pasaba con la córnea, es necesario que los elementos que forman el cristalino tengan un índice de refracción mayor que el de la córnea y del humor acuoso, lo que le permitirá enfocar correctamente.</p>
<p>Una ves atravesado el cristalino, la luz llegará a la <strong>retina</strong> donde se producirá la transformación de la luz en impulsos eléctricos, que posteriormente viajarán por el nervio óptico hasta el cerebro, que será capaz de procesar y comprender la imagen recibida.</p>
<h3 id="la-retina-y-su-importancia"><span class="header-section-number">2.1.1</span> La retina y su importancia</h3>
<!-- 1 día
     Comienzo: 15 Mayo
     Fin V1: 18 Mayo
     Papers útiles: (P0) (P7) (P8)
     Webs:
-->
<p>La palabra <strong>retina</strong> procede del latín mediaval <strong>rete</strong> o <strong>retis</strong> (red). Toma ese nombre, debido a la gran red de vasos sanguíneos que la forman.</p>
<p>Utilizando términos de ingeniería, la retina es el transductor en el proceso de visión. Es la capa de tejido sensible a la luz, situada en el fondo del ojo, sin la cual todo el proceso detallado anteriormente carecería por completo de sentido. Su color, rojo, es debido a la inmensa cantidad de vasos sanguíneos que existen detrás de ella.</p>
<p>A nivel macroscópico, la retina está formada por los siguientes elementos:</p>
<ul>
<li><strong>Papila o disco óptico</strong>: Es el conocido como <em>punto ciego</em>, debida a la ausencia de fotoreceptores. Es el punto de entrada del nervio óptico en el globo ocular</li>
<li><strong>Arterias y venas</strong>: Encargadas de proveer de oxígeno y nutrientes a la retina. La arteria central de la retina entre en el ojo a través del nervio óptico y se separa en dos ramos, que a su vez se separarán formando una extensa red de capilares.</li>
<li><strong>Mácula</strong>: Esta pequeña área, con gran pigmentación, se encuentra en el centro de nuestra retina. La mácula tiene un diámetro aproximado de 5 mm. Es la encargada tanto la visión central como de la visión en detalle y en movimiento.</li>
<li><strong>Fóvea</strong>: Es una hendidura en el centro de la mácula, con un diámetro aproximado de 1.0 mm, que permite enfocar los rayos que llegan a la retina.</li>
<li><strong>Retina periférica</strong>: Como su nombre indica, nos permite la visión periférica, es decir, la de los rayos de luz que no están en nuestro foco central de visión.</li>
</ul>
<p>A nivel microscópico, la retina tiene una estructura compleja formada por varias capas de neuronas interconectads. Existen dos tipos principales de fotoreceptores en la retina: los conos y los bastones. Las células de la retina presentan grandes similitudes con las del cerebro, apoyando la afirmación común de que el sistema visual es una extensión del sistema nervioso cetral <span class="citation" data-cites="zhu2001eye">(Zhu et al. 2001)</span>.</p>
<p>Estos receptores contienen unos productos químicos conocidos como fotopigmentos. Los fotopigmentos tienen la propiedad de descomponerse ante la exposición a la luz, excitando en el proceso a las fibras nerviosas que salen del ojo.</p>
<p>Los bastones son estructuras cilíndricas y alargadas que son extremedamente sensibles a los cambios de intensidad de de la luz. Sin embargo, no son capaces de percibir información sobre el color. De esto se encargan los conos, que son células más pequeñas y finas capaces de de percibir el color y de capturar detalles más finos.</p>
<p>En los conos encontramos tres tipos distintos de fotopigmentos que responden a diferentes longitudes de onda distinta de la luz. Esto da lugar a los conocidos como 3 colores primarios de la luz: rojo, azul, y verde.</p>
<p>En la fóvea central, los únicos fotoreceptores existentes son los conos, encargados de la visión en detalle y visión en color. Según nos alejamos de la fóvea y nos dirigimos hacia la parte más periférica de la retina, los bastones empiezan a ser predominantes. Estos son responsables de la visión periférica y la visión en bajas condiciones de luminosidad.</p>
<p>En la retina humana existen aproximadamente 125 millones de fotoreceptores, de los cuales, aproximadamente 120 millones son bastones y 5 millones son conos</p>
<h2 id="principales-patologías-de-la-retina"><span class="header-section-number">2.2</span> Principales patologías de la retina</h2>
<!-- 1 día
    Comienzo: 18 Mayo
    Fin V1:
    Papers útiles:
-->
<p>Existen dos tipos principales de enfermedades que afectan a la retina: las enfermedades vasculares y las degenerativas. Durante este trabajo analizaremos dos de las más importantes: la Retinopatía Diabética (RD) y la Degeneración Macular Asociada a la Edad (DMAE).</p>
<p>Aún siendo de naturaleza distinta y provocando distintos efectos, ambas patologías tienen algo en común: la mayoría de casos de ceguera provocados por ellas hubieran sido evitables con una detección y tratamiento de las mismas en los primeros estadios. La detección de estas, como veremos más adelante, pasa comunmente por el análisis de la retina mediante imágenes de fondo de ojo.</p>
<h3 id="retinopatía-diabética"><span class="header-section-number">2.2.1</span> Retinopatía diabética</h3>
<!-- 1 día
    Comienzo:
    Fin V1:
    Papers útiles: (P1)
-->
<p>La Retinopatía Diabética (RD) pertenece al grupo de las enfermedades vasculares, y se ha convertido en la principal causa evitable de ceguera en todo el mundo. Esta patología se da en el 35% de las personas con diabetes, que afecta al 8.5% de la población mundial <span class="citation" data-cites="idf2017">(Anon n.d.a)</span> <span class="citation" data-cites="IAPB">(Anon n.d.b)</span>. Se estima que 191 millones de personas sufrirán retinopatía diabética en 2030 <span class="citation" data-cites="zheng2012worldwide">(Zheng et al. 2012)</span>.</p>
<p>La diabetes supone, aproximadamente, el 11.6% del presupuesto total de salud de la mayoría de países [@{zhang2009economic]. Además, el coste de los pacientes con RD supera notablemente al de los pacientes sin dicha patología, incrementándose con la gravedad de la retinopatía <span class="citation" data-cites="zhang2017direct">(Zhang et al. 2017)</span>.</p>
<p>Las personas que sufren de diabetes, presentan altos niveles de azucar en sangre debido a la incapacidad de su páncreas de generar suficiente insulina para distribuir el azucar (Diabetes Tipo I) o a la incapacidad del organismo de asimilar correctamente la insulina (Diabetes Tipo II). Estos altos niveles de azucar pueden producir daños en varios organismos presentes en nuestro cuerpo.</p>
<p>La retinopatía diabética ocurre cuando, debido a la diabetes, se dañan los vasos sanguíneos de la retina. Es común establecer dos etapas principales de RD: proliferativa y no proliferativa.</p>
<ul>
<li><strong>DR No Proliferativa (NPDR)</strong>: Durante etapa aparecen microaneurismas, pequeñas áreas de inflamación en los vasos sanguíneos de la retina. Además, algunos vasos sanguíneos se obstruyen. En casos más complicados, el bloqueo de una gran cantidad de vasos sanguíneos provoca que haya áreas de la retina que dejen de recibir sangre por completo.</li>
<li><strong>DR Proliferativa (PDR)</strong>: En esta etapa, de mayor gravedad, las áreas de la retina que no estaban recibiendo sangre, envían señales al cuerpo para que se hagan crecer nuevos vasos sanguíneos. Sin embargo, estos nuevos vasos sanguíneos son frágiles y anormales, y en el caso de rotura y goteo de sangre, podrían provocar una pérdida severa en la visión o incluso resultar en la ceguera.</li>
</ul>
<p>La detección temprana de la RD ha demostrado ser de vital importancia para evitar la pérdida de vista e incluso la ceguera causada por la misma. El tratamiento de la retinopatía diabética más común es la <strong>fotocoagulación con láser</strong>. Este tratamiento se puede realizar en una o varias sesiones, tras haber comprobado, mediante una angiografía fluoresceínica el estado de los vasos sanguíneos. Además, este tratamiento puede ir acompañada de inyecciones intravítreas de medicación antangiogénica, que se encargará de evitar el desarrollo excesibo y anormal de los vasos sanguíneos. En casos de gravedad, puede ser preciso recurrir a la vitrectomía, una técnica de microcirugía intraocular.</p>
<p>La detección de la retinopatía diabética en las primeras etapas se basa en la aparición en las imágenes de fondo de ojo de pequeños puntos rojos edbidos a microaneurismas o, incluso, hemorragias.</p>
<p>Las lesiones típicas derivadas de la retinopatía diabética son:</p>
<ul>
<li><strong>Exudados duros</strong>: Son depósitos lipídicos de color amarillento brillante y bien definidos</li>
<li><strong>Exudados blandos</strong>: Son engrosamientos isquémicos de la capa de fibras nerviosas. Presentan bordes difusos y un color blanco.</li>
<li><strong>Microaneurismas</strong>: Aparcen normalmente como pequeños grupos de puntos rojos con bordes muy definidos. Son causados por la dilatación de pqueñas venas, y son uno de los primeros signos de retinopatía diabética no proliferativa.</li>
<li><strong>Hemorragias</strong>: Son pequeñas manchas con diversas formas que aparecen en las imágenes de fondo de ojo debido a los puntos de sangrado en la retina.</li>
</ul>
<p>La tabla  nos muestra una posible clasificación de los diferentes estadios de la retinopatía diabética en función de las lesiones presentes en el paciente:<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<table>
<caption>Niveles de gravedad de la retinopatía diabética en función de las lesiones observadas </caption>
<colgroup>
<col style="width: 39%" />
<col style="width: 60%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Nivel de gravedad</th>
<th style="text-align: left;">Observaciones</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Grado 0</strong>: No RD</td>
<td style="text-align: left;">Sin ninguna anomalía</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Grado 1</strong>: NPDR Ligera</td>
<td style="text-align: left;">Presencia de algunos microaneurismas</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Grado 2</strong>: NPDR Moderada</td>
<td style="text-align: left;">Presencia de más microaneurismas pero menos que en el grado 3</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Grado 3</strong>: NPDR Severa</td>
<td style="text-align: left;">Alguno de los siguientes:</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">- Más de 20 hemorragias intrarretinales</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">- Dilataciones venosas</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">- Anomalías microvasculares en la retina</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">- Ningún signo de PDR</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Grado 4</strong>: PDR</td>
<td style="text-align: left;">Alguno de los siguientes:</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">- Neovascularización</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">- Hemorragia vítrea</td>
</tr>
</tbody>
</table>
<!-- TODO: Imagenes de fondo de ojo con distintos tipos de lesiones etc -->
<h3 id="degeneración-macular-asociada-a-la-edad"><span class="header-section-number">2.2.2</span> Degeneración macular asociada a la edad</h3>
<!-- 1 día
    Comienzo: 18 Mayo
    Fin V1:
    Papers útiles:
-->
<p>La Degeneración Macular Asociada a la Edad (DMAE) es la más común de las enfermedades que afectan a la retina. Esta patología, de tipo degenerativo, afecta a la mácula provocando que, quien la sufre, comience a ver imágenes desenfocadas o deformadas y con zonas oscurecidas. Como se ha explicado previamente, la mácula permite la visión central, y su degeneración afecta directamente al día a día del paciente incapacitándolo para hacer tareas comunes como puede ser la lectura o la conducción</p>
<p>La DMAE is la mayor causa de ceguera en países desarrollados, dándose en un 9% de la población mundial <span class="citation" data-cites="wong2014global">(Wong et al. 2014)</span>. Hasta el 80% de los casos de ceguera causados por esta enfermedad son evitables si son detectados y tratados a tiempo.<span class="citation" data-cites="pascolini2012global">(Pascolini &amp; Mariotti 2012)</span></p>
<p>Aunque en sus primeros estadios, la progresión de la enfermedad sea muy lenta y el paciente puede que únicamente perciba un ligero cambio en su visión, en fases avanzadas la DMAE puede provocar la pérdida total de la visión central.</p>
<p>Si es detectada a tiempo, la DMAE puede ser retardada y mitigada mediante vitaminas y minerales.</p>
<p>El principal signo de DMAE en las imágenes de fondo de ojo es la apaición de las <strong>drusas</strong>, depósitos amarillos localizados bajo la retina, procedentes de la acumulación de minerales. En función del número y tamaño de drusas, podemos definir tres estadios en la enfermedad:</p>
<ul>
<li><strong>Estadio inicial</strong>: En este estadop existe un reducido número de drusas redondas y de pequeño tamaño (menos de 125 μm). Además, estas tienen unos bordes bien definidos. Los pacientes con este grado de DMAE no sufren pérdida de visión y tienen un riesgo bajo de desarrollar complicaciones.</li>
<li><strong>Estadio intermedio</strong>: En este estadio existen muchas más drusas, y estas tienen un tamaño mayor, llegando incluso a aparecer algunas de más de 125 μm. Se pueden apreciar cambios en la pigmentación de la retina.</li>
<li><strong>Estadio avanzado</strong>: Existen dos subniveles dentro del estadio avanzado:
<ul>
<li><strong>DMAE seca o atrófica</strong>: Se produce por la acumulación de desechos que atrofian la zona macular. Es la forma más común, y tiene una evolución lenta y progresiva.</li>
<li><strong>DMAE húmeda o exudativa</strong>: Es la DMAE húmeda, crece una membrana vascular bajo la retina. Estos nuevos vasos sanguíneos son muy frágiles y pueden romperse derramando líquido que afectará severamente a la visión.</li>
</ul></li>
</ul>
<!-- TODO: Imagenes de fondo de ojo de DMAE -->
<h3 id="sistemas-de-diagnóstico"><span class="header-section-number">2.2.3</span> Sistemas de diagnóstico</h3>
<!--
    Comienzo: 18 Mayo
    Fin V1:
    Papers útiles:
    Webs:
-->
<p>Uno de los grandes problemas de la retinopatía diabética es que no existe ninguna señal que nos avise en estadios muy tempranos de la enfermedad, y en el momento que los usuarios deciden hacerse una examinación, suele ser demasiado tarde para un tratamiento óptimo.</p>
<p>Existen varios tipos de sistemas para el diagnóstico de la retinopatía diabética y la degenración macular, entre los que destacan las fotografías de fondo de ojo, la tomografía de coherencia óptica (OCT) o la angiografía. Sin embargo, la fotografía de fondo de ojo puede ser realizada con sistemas relativamente baratos y fáciles de manejar y de transportar. Además, las cámaras de fondo de ojo pueden capturar la información de la retina mediante técnicas no invasivas. Es por ello, por lo que es el sistema más utilizado en los centros de atención primaria y en el que enfocaremos nuestro estudio. Sin embargo, cabe destacar que, en ocasiones, éstas no son suficientes, y tendrán que ser combinadas con otros tipos de sistemas.</p>
<p>El diagnóstico de la retinopatía diabética es tradicionalmente realizado por oftalmólogos que inspeccionan las imágenes de fondo de ojo en busca de las diferentes lesiones que caracterizan estas patologías. Sin embargo, este es un proceso que requiere una gran cantidad de tiempo. La limitada cantidad de profesionales capaces de realizar este proceso hace imposible cubrir la demanda actual, que no hace más que crecer <span class="citation" data-cites="bjorvig2002economic">(Bjørvig et al. 2002)</span>. Este hecho es más acusado en zonas rurales o países no desarrollados donde no es posible el acceso a este tipo de profesionales. El 75% de los pacientes de retinopatía diabética viven en áreas donde no existen especialistas ni infraestructura para la detección y tratamiento de la enfermedad <span class="citation" data-cites="guariguata2014global">(Guariguata et al. 2014)</span>. Por ello, herramientos para el análisis automático de retina (ARIA) están siendo desarrolladas actualmente. En este tipo de herramientas nos centraremos durante el desarrollo de este trabajo. Este tipo de herramientas llevarán el diagnóstico a sitios donde no sería posible de otra forma, además de reducir costes y reducir el tiempo necesario para los diagnósticos en los lugares donde ya se realizaba de forma manual.</p>
<p>Además, según se ha puesto de manifiesto en previos estudios, los profesionales difieren en numerosas ocasiones en el diagnóstico de los diferentes estados de este tipo de patologías, debido a que existe un cierto grado de subjetividad <span class="citation" data-cites="sellahewa2014grader">(Sellahewa et al. 2014)</span> <span class="citation" data-cites="ruamviboonsuk2005screening">(Ruamviboonsuk et al. 2005)</span>.</p>
<p>Las técnicas de ARIA se basaron en los últimos años en la extracción de manual de caráctrísticas de las imágenes de fondo de ojo, que posteriormente se le pasarían a un clasificador de Machine Learning. Era, por lo tanto, un proceso con dos fases claramente diferenciadas, y que requería de conocimiento experto para la definición de las características que nos fueran de mayor utilidad para la detección de la RD y la DMAE.</p>
<p>Con la entrada del conocido como Deep Learning, y concretamente en este caso las Redes Neuronales Convolucionales (CNN) este proceso inicial de extracción de características ha podido ser automatizado, mejorando la calidad de las evaluaciones notablemente. Las fases de extracción de carácterísticas y de predicción se unifican. Las características extraídas poseen un poder mucho mayor de predicción, puesto que toda la red ha sido entrenada para ello.</p>
<p>Sin embargo, dada la naturaleza del problema, la falta de estandarización y la escasa cantidad de imágenes etiquetadas, ha provocado que estos sistemas tuvieran serias dificultades para su aplicación general.</p>
<p>Esta aproximación al problema del diagnóstico de la RD y DMAE plantea una serie de preguntas. ¿Cómo se incorporaría un sistema de este tipo en las consultas? ¿Se introduciría el software en las propias cámaras que captan la imagen de la retina? ¿Sería posible crear en los paises un sistema centralizado de diagnóstico con imágenes de fondo de ojo? ¿Podrían utilizarse en este sistema centralizado técnicas ARIA combinadas con las opiniones de expertos? No hay una respuesta para todas estas preguntas y las ventajas e inconvenientes de cada una de ellas, se desarrollarán en capítulos posteriores.</p>
<h1 id="machine-learning-y-aplicaciones-médicas"><span class="header-section-number">3</span> Machine Learning y aplicaciones médicas</h1>
<!-- Comienzo: Lunes, 27 Mayo
     Previsión: Lunes, 3 de Junio
     Papers útiles: (P2) (P0) (P25-P36)
     Papers terminados: P36
-->
<p>Tradicionalmente, el trabajo de los ingenieros de software ha consistido en dar a las computadoras una serie de reglas explícitas de cómo tiene que procesar la información y cómo tiene que tomar decisiones. Sin embargo, la complejidad del campo de la medicina es tal que sería prácticamente imposible capturar toda la información relevante mediante una serie de reglas definidas de forma explícita <span class="citation" data-cites="schwartz1986artificial">(Schwartz et al. 1986)</span>.</p>
<p>El Machine Learning ha permitido crear sistemas que aprendan de los ejemplos sin necesidad de que se programen reglas específicas, lo que ha supuesto una auténtica revolución en prácticamente cualquier sector profesional imaginable entre los que, por supuesto, se encuentra también la medicina. Estos sistemas buscan, de forma automática, la mejor forma de predecir una variable en función de una serie de variables de entrada del sistema. De esta forma se crea un sistema que, idealmente, será capaz de obtener la salida correcta para variables de entrada nunca vistas. Esto se conoce como <strong>aprendizaje supervisado</strong> aunque es importante mencionar, que no es la única forma de Machine Learning o Aprendizaje Automático.</p>
<p>El principal inconveniente del Machine Learning con respecto al aprendizaje humano es, a la vez, su principal ventaja: la necesidad de grandes cantidades de datos para su correcto funcionamiento.. Si se alimentan con una cantidad suficiente de datos, los algoritmos de Machine Learning podrán encontrar patrones que para los humanos sería prácticamente imposible. Un modelo de Machine Learning podrá analizar en segundos más pacientes de los que verá un médico en toda su vida. Además, la cantidad de predictores distintos que manejará sería totalmente inviable para un humano.</p>
<h2 id="ia-big-data-machine-learning-y-deep-learning"><span class="header-section-number">3.1</span> IA, Big Data, Machine Learning y Deep Learning</h2>
<p>Inteligencia Artificial, Big Data, Machine Learning, Deep Learning; actualmente existe mucha confusión en el uso de estos términos. Aunque comparten características, no tienen el mismo significado. En este capítulo se detallarán las similitudes y diferencias entre todos ellos para evitar el lenguaje inexacto usado habitualmente en este campo.</p>
<p>Comenzaremos por el Big Data, pues es el término más vago y confuso. Cuando hablamos de Big Data nos referimos al análisis de grandes cantidades de datos que no podrían ser analizados con técnicas convencionales. Sin embargo, las líneas que marcan las fronteras del Big Data están difusas, y a menudo es un término utilizado más por medios de comunicación y gurús que por profesionales técnicos y académicos.</p>
<p>Por otro lado, los campos de la Inteligencia Artificial, el Machine Learning y el Deep Learning, sí que están más claramente definidos aunque, el hecho de que cada uno de ellos sea un subcampo del anterior, a menudo da lugar a confusión. Llamamos Inteligencia Artificial a un conjunto de técnicas que tratan de que los ordenadores imiten, de alguna forma, el comportamiento humano.</p>
<p>El Machine Learning es un subcampo dentro de la IA, que consiste en un conjunto de técnicas y herramientas que permiten a los ordenadores obtener patrones de grandes conjuntos de datos. Gracias a esos patrones seremos capaces de entender mejor los datos o incluso hacer predicciones. La forma más común de machine learning es el conocido como <strong>Aprendizaje Supervisado</strong>. Durante el entrenamiento de los modelos de Aprendizaje Supervisado, se proporcionan al algoritmo una serie de datos históricos. Entre ellos se encuentra la <strong>variable objetivo</strong>, es decir, la que posteriormente querremos predecir en los nuevos datos. Por ejemplo, en un modelo de detección de cáncer a partir de imágenes médicas, nuestra variable objetivo será precisamente la que indique si una imágen pertenece a un paciente enfermo o un paciente sano. Esta variable, por lo tanto, tendrá dos posibles valores siendo este un problema de <strong>clasificación</strong>. En los problemas de clasificación se tratan de predecir variables discretas. Si, por ejemplo, realizáramos un modelo para predecir el precio de una vivienda en función de sus características, nos encontraríamos ante un problema de regresión, pues la probabilidad es un valor contínuo.</p>
<p>Es común en los algoritmos para aprendizaje supervisado el uso de una función de pérdidas. Esta función mide el error entre las predicciones del modelo y los datos reales. De forma iterativa, los algoritmos tratarán de ajustar una serie de parámetros (o pesos) intentando minimizar esta función.</p>
<p>Precisamente estos últimos algoritmos, las redes neuronales, son las que dan lugar al Deep Learning. Cuando añadimos más capas intermedias a las redes neuronales somos capaces de detectar patrones mucho menos evidentes además de tratar problemas complejos sin necesidad de un pre-procesamiento manual previo que los simplifique. Los algoritmos de Deep Learning son actualmente el estado del arte en tareas como reconocimeiento de imágenes <span class="citation" data-cites="krizhevsky2012imagenet">(<span class="citeproc-not-found" data-reference-id="krizhevsky2012imagenet"><strong>???</strong></span>)</span>, reconocimiento del habla <span class="citation" data-cites="deng2013new">(<span class="citeproc-not-found" data-reference-id="deng2013new"><strong>???</strong></span>)</span>, procesamiento del lenguaje natural <span class="citation" data-cites="collobert2011natura">(<span class="citeproc-not-found" data-reference-id="collobert2011natura"><strong>???</strong></span>)</span>, análisis de información de aceleradores de partículas <span class="citation" data-cites="baldi2014searching">(<span class="citeproc-not-found" data-reference-id="baldi2014searching"><strong>???</strong></span>)</span> o reconstrucción de los circuitos cerebrales <span class="citation" data-cites="helmstaedter2013connectomic">(<span class="citeproc-not-found" data-reference-id="helmstaedter2013connectomic"><strong>???</strong></span>)</span>, entre muchas otras.</p>
<h2 id="redes-neuronales-descenso-de-gradiente-y-backpropagation"><span class="header-section-number">3.2</span> Redes neuronales, descenso de gradiente y backpropagation</h2>
<p>Una red neuronal consiste en un conjunto de nodos, conocidis como <strong>neuronas</strong>, conectados entre si para transmitirse señales.</p>
<p>Estas neuronas están dispuestas en una serie de capas, en las que cada neurona de una capa está conectado a todas las neuronas de las capas anteriores. Cada neurona combina sus entradas con un conjunto de coeficientes o pesos. El nombre de <strong>peso</strong> se debe a que la función de estos, al multiplicarse por los valores de las entradas es definir la importancia de cada una. Los resultados de todos estos productos se suman y se pasa el resultado a lo que se conoce como <strong>función de activación</strong>, que añade un comportamiento no-lineal al proceso. Actualmente, la función de activación más conocida es la <strong>ReLU</strong> (Rectified Linear Unit) cuya fórmula es simplemente <span class="math inline"><em>f</em>(<em>z</em>) = <em>m</em><em>a</em><em>x</em>(<em>z</em>, 0)</span>.</p>
<p>Durante el entrenamiento, los pesos cambian de valor, intentand minimizar la función de pérdidas explicada en el apartado anterior. Para ajustar el vector de pesos se suele calcular el vector de gradiente que indica, para cada peso cómo se modificaría el error si ese peso se aumentara ligeramente. El vector de pesos es entonces ajustado en el sentido opuesto al vector de gradiente. Esto es lo que conocemos como <strong>descenso de gradiente</strong>. En la práctica, este proceso no usa todos los datos cada vez sino que se utiliza el <strong>Descenso de Gradiente Estocástico</strong> (SGD por sus iniciales en inglés). Gracias al SGD podemos actualizar los pesos de nuestra red neuronal tomando cada vez un pequeño conjunto de datos (conocido como <strong>batch</strong>).</p>
<p>El origen de las redes neuronales es el <strong>Perceptrón</strong> desarrollado en los años 40 que eran redes simples de una sola capa de entrada y una capa de salida. Sin embargo, fue en los años 80 cuando estas comenzaron a desarrollar su verdadero potencial gracias al algoritmo de backpropagation, que permitió que se añadieran nuevas capas intermedias a las redes neuronales conocidas como <strong>capas ocultas</strong>. La técnica de backpropagation no es más que una aplicación de la regla de la cadena de las derivadas. Gracias a ella podemos propagar el error a lo largo de las capas, para calcular en cada una el vector de gradiente y actualizar con él los pesos.</p>
<p>Estas nuevas capas intermedias añadidas a las redes neuronales permitían encontrar patrones más complejos, y dieron lugar a lo que conocemos como <strong>Deep Learning</strong>. Estrictamente hablando, nos referimos a Deep Learning cuando tenemos una red con más de una capa oculta. El Deep Learning permite crear modelos computacionales compuestos de múltiples capas de procesamiento que son capaces de aprender representaciones de los datos con múltiples capas de abstracción <span class="citation" data-cites="lecun2015dee">(<span class="citeproc-not-found" data-reference-id="lecun2015dee"><strong>???</strong></span>)</span>.</p>
<p>En las redes profundas, cada capa de neuronas se entrena en un conjunto de características distintos, en base a la salida de la capa anterior. A medida que avanzamos a través de la red, las características que las neuronas son capaces de detectar son más complejas, ya que agregan y recombinan características de capas anteriores. Esta propiedad, conocidad como jerarquía de características hace posible que este tipo de redes sean capaces de tratar datasets de muy alta dimensionalidad. Las redes neuronales profundas realizan una <strong>extracción automática de características</strong>, sin la necesidad de la intervención de un humano.</p>
<h3 id="redes-neuronales-convolucionales"><span class="header-section-number">3.2.1</span> Redes neuronales convolucionales</h3>
<p>La capacidad de las redes neuronales de encontrar patrones complejos en datasets con una gran cantidad de dimensiones las convierten en candidatas perfectas para tareas como la clasificación de imágenes o el reconocimiento de voz. Sin embargo, y como hemos visto a lo largo de este capítulo, estos clasificadores necesitan un trabajo manual previo de extracción de características.</p>
<p>La aparición de las conocidas como <strong>Redes Convolucionales (CNN)</strong> permitió eliminar este paso y delegarlo en el propio algoritmo de backpropagation. De esta forma, es posible usar como entradas de nuestro modelo los <em>datos en bruto</em> (píxeles de las imágenes, muestras de las pistas de audio, etc). Un momento clave para las redes convolucionales fue en 2012, en el ImageNet Large Scale Visual Recognition Challenge (ILSVRC) cuando una solución novedosa basada en CNNs <span class="citation" data-cites="krizhevsky2012imagenet">(<span class="citeproc-not-found" data-reference-id="krizhevsky2012imagenet"><strong>???</strong></span>)</span> obtuvo, de forma holgada, la primera posición.</p>
<p>Una de las principales ventajas de las redes neuronales convolucionales con respecto a otras aproximaciones al problema es que poseen un cierto grado de invarianza a la distorsión y al desplazamiento. Esto permite que podamos usar este tipo de redes sin apenas pre-procesamiento.</p>
<p>La arquitectura de las redes convolucionales está basada en la organización de la corteza visual del cerebro humano. En él existen neuronas individuales que responden a estímulos en una región delimitada del campo visual.</p>
<p>Las redes convolucionales constan de capas convolucionales y de reducción (o pooling) alternadas.</p>
<ul>
<li>En las capas de convolución de aplican una serie de filtros (cuyos pesos son parámetros modificados durante el entrenamiento por el backpropagation). En ellas se producen también las transformaciones no lineales (ReLU). Cada uno de los filtros se desplazará sobre toda la imagen calculándose en cada posición el producto vectorial entre la región de la imagen y los valores del filtro. Estos filtros hacen de <strong>detectores de características</strong>.</li>
<li>En las capas de reducción o pooling se disminuye la cantidad de parámetros. Para ello se obtiene el promedio o el máximo de una serie de regiones.</li>
</ul>
<p>Sobre todas estas capas tenemos las <strong>Fully Connected Layer</strong>, redes tradicionales que a partir de los parámetros extraídos por las capas convolucionales y de pooling, realizan las clasificaciones o regresiones finales.</p>
<h3 id="arquitecturas-de-redes-neuronales"><span class="header-section-number">3.2.2</span> Arquitecturas de redes neuronales</h3>
<h2 id="medidas-de-evaluación-de-sistemas-de-deep-learning"><span class="header-section-number">3.3</span> Medidas de evaluación de sistemas de Deep Learning</h2>
<!--
    Comienzo: 189 Mayo
    Fin V1:
    Papers útiles: (P2 pag 112)
    Webs:
-->
<p>Un paso tan importante como el modelado, en un proyecto de análisis de datos, es la evaluación de los resultados. Es de vital importancia establecer medidas que nos permitan saber cómo se está comportando nuestro modelo.</p>
<p>En la literatura, existen una extensa cantidad de medidas, aunque en este caso nos centraremos en algunas de las más comunes en problemas de este tipo.</p>
<p>El problema que se está analizando en este trabajo es un <strong>problema binario</strong>, es decir se trata de predecir una clase con sólo dos posibles valores, verdadero o falso. Más concretamente, únicamente trataremos de detectar si la persona tiene o no la enfermedad. Cuando, en un problema de este tipo, comparamos la predicción realizada por un modelo, con el <em>ground truth</em> (es decir, la clase que realmente correspondería a esa instancia), pueden darse 4 posibles casos:</p>
<ul>
<li><strong>Verdadero Positivo (o True Positive, TP)</strong>: El sistema predice que el paciente <strong>SÍ</strong> tiene la enfermedad y acierta.</li>
<li><strong>Verdadero Negativo (o True Negative, TN)</strong>: El sistema predice que el paciente <strong>NO</strong> tiene la enfermedad y acierta.</li>
<li><strong>Falso Negativo (o False Negative, FN)</strong>: EL sistema predice, erróneamente, que el paciente <strong>NO</strong> tiene la enfermedad cuando en realidad sí que la tiene.</li>
<li><strong>Falso Positivo (o False Positive, FP)</strong>: El sistema predice, erróneamente, que el paciente <strong>SÍ</strong> tiene la enfermedad cuando en realidad no la tiene.</li>
</ul>
<p>A partir de la cantidad de predicciones de cada uno de estos posibles 4 tipos se pueden definir una serie de medidas muy comunes en problemas de este tipo.</p>
<p>La <strong>sensitividad</strong>…</p>
<h2 id="transfer-learning"><span class="header-section-number">3.4</span> Transfer Learning</h2>
<p>La mayoría de métodos de Machine Learning asumen que los datos de entrenamiento y los de test vienen de la misma distribución y espacio funcional. <span class="citation" data-cites="pan2009survey">(Pan &amp; Yang 2009)</span>. Por ello, cuando esta distribución cambia, debemos volver a entrenear nuestros modelos desde 0, obteniendo datos totalmente nuevos. El Transfer Learning, sin embargo, nos permite tener distribuciones distintas en entrenamiento y test, mediante la transferencia de conocimiento entre modelos.</p>
<p>El Transfer Learning es una técnica de Machine Learning que permite utilizar un modelo desarollado para una tarea específica como punto de partida para otra tarea distinta (aunque relacionada). Además de permitirnos obtener clasificadores de forma mucho más rápida aprovechando el conocimiento previo, el Transfer Learning hace posible el uso del Deep Learning con conjuntos de datos pequeños con los que sería imposible entrenar una red desde 0. El Transfer Learning es considerado por muchos investigadores como un paso más en dirección hacia la AGI.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>Aunque el Deep Learning es usado en diversidad de tareas como Procesamiento del Lenguaje Natural o tratamiento de audio, en este capítulo se analizará su uso en Visión por Computador, que es el caso adecuado a nuestro problema.</p>
<h3 id="transfer-learning-con-imágenes"><span class="header-section-number">3.4.1</span> Transfer Learning con imágenes</h3>
<p>En la práctica, muy poca gente entrena redes convolucionales desde 0. Existen 2 principales motivos:</p>
<ul>
<li>En determinados ámbitos, no siempre existen datasets con una gran cantidad de imágenes, suficiente para entrenar una red desde 0.</li>
<li>Aún existiendo dicho dataset, el tiempo necesario para su completo entrenamiento puede ser de días, semanas o incluso meses dependiendo del equipo usado.</li>
</ul>
<p>Existen tres principales estrategias a la hora de realizar Transfer Learning:</p>
<ul>
<li><strong>Red convolucional como extractor de características</strong>: Como se ha analizado anteriormente una red convolucional puede ser vista como una herramienta para extraer características de las imágenes, que posteriormente serán usadas por capas totalmente conectadas (o por cualquier otro tipo de clasificador) para realizar la clasificación. Conociendo esto, podemos utilizar la red convolucional entrenada para un conjunto de imágenes, en otro conjunto de imágenes distinto, siendo el clasificador final el único que tendrá que reentrenarse.</li>
<li><strong>Fine-tunning de la red convolucional</strong> Como se ha analizado anteriormente, las capas iniciales de las redes convolucionales se encargan de detectar características más generales y patrones simples, que van siendo más complicados a medida que avanzamos hacia capas posteriores. Por lo tanto, es común que estas primeras capas tengan siempre contenidos similares incluso en modelos entrenados con diferentes conjuntos de imágenes. Por lo tanto, podrán ser reaprovechadas, con lo que únicamente tendremos que reentrenar las últimas capas y el clasificador final.</li>
<li><strong>Modelos pre-entrenados</strong>: Este tercer caso supone el reentrenamiento total de la red, sin embargo, partiendo de unos pesos que han sido previamente entrenados en otro conjunto de imágenes. De esta forma conseguimos que el número de iteraciones necesarias hasta llegar al nivel de exactitud requerido sea menor.</li>
</ul>
<p>Los criterios para decidir qué estrategia de Transfer Learning usar en cada caso dependen principalmente de las diferencias de contenido y tamaño entre las imágenes de nuestro dataset y las del dataset original (con el que se entrenó el modelo que vamos a reutilizar)</p>
<p>Es común usar las siguientes reglas como guía en función de 4 posibles escenarios:<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<ul>
<li><strong>El nuevo dataset es pequeño pero similar al original</strong>: Al ser un dataset muy pequeño, modificar las capas convolucionales de nuestro modelo original puede dar lugar a overfitting. Por lo tanto, y puesto que las imágenes de ambos datasets son similares, la estrategia adecuada será utilizar la red convolucional como extractor de características y entrenar únicamente el clasificador final.</li>
<li><strong>El nuevo dataset es grande y similar al original</strong>: En este caso, como tenemos más imágenes podremos realizar fine-tunning de la red sin miedo a caer en overfitting.</li>
<li><strong>El nuevo dataset es pequeño y muy diferente al original</strong>: De nuevo, al tener un dataset pequeño, descartaremos entrenar la red convolucional. En este caso, lo que haremos es entrenar solo un clasificador. Y además, al ser las imágenes distintas a las del dataset original, no podremos aprovechar las últimas capas de la red convolucional que serán eliminadas.</li>
<li><strong>El nuevo dataset es grande y muy diferente al original</strong>: En este caso entrenaremos la red convolucional al completo. Sin embargo, será de gran utilidad comenzar nuestro entrenamiento a partir de un modelo pre-entrenado.</li>
</ul>
<h2 id="aplicaciones-médicas-del-deep-learning"><span class="header-section-number">3.5</span> Aplicaciones médicas del Deep Learning</h2>
<p>¿Cómo sería un sistema sanitario en el que cada decisión relacionada con una enfermedad, en lugar de ser tomada por una sola persona, fuera tomada por un conjunto de los principales expertos del mundo de esa enfermedad? Esa es la pregunta que se hacen multitud de investigadores <span class="citation" data-cites="rajkomar2019machine">(Rajkomar et al. 2019)</span>. Estos concluyen que las tratamientos recetados serán los más efectivos, y no, los más conocidos por la persona que los prescribe. Además, se evitaría el error humano. Por desgracia, un sistema de este tipo sería inviable, debido principalmente a la falta de expertos, que no darían abasto para diagnosticar a millones de pacientes cada día. Sin embargo, el Machine Learning nos promete un sistema similar a este, pero realmente viable y escalable. Con la capacidad de aplicar todas las lecciones recogidas de la experiencia colectiva en cada una de las decisiones, sin que esto genere una gran carga de trabajo para unos pocos expertos.</p>
<p>Hace ya 50 años se ponía de manifiesto la necesidad de “aumentar, o incluso remplazar las funciones intelectuales de los médicos” <span class="citation" data-cites="schwartz1970medicine">(Schwartz 1970)</span>.</p>
<p>El uso de herramientas estadísticas en medicina no es ninguna novedad. Desde antes de la irrupción de las técnicas más novedosas de Machine Learning y Deep Learning, la estadística descriptiva tenía un papel fundamental, estando prácticamente siempre presente en los artículos de las revistas de medicina. Son necesarias técnicas estadísticas que nos permitieran estudiar la eficacia de los fármacos o los factores de riesgo de determinadas enfermedades.</p>
<p>La rama de la <strong>epidemilogía</strong>, cuyos orígenes se sitúan hacia el siglo IV a.C., trata de recopilar y tratar los datos de los pacientes y sus patologías, para estudiar la frecuencia y distribución de los diversos fenómenos relacionados con la salud. La epidemilogía trata de encontrar patrones en las enfermedades centrándose principalmente en tres aspectos: tiempo, lugar y persona. Gracias a ella somos capaces de definir los problemas de salud más importantes de una comunidad, además de sus factores de riesgo. Con esta información podremos desarrollar programas de prevención o control, e incluso predecir tendencias de una enfermedad.</p>
<p>Sin embargo, la revolución del Machine Learning y el Deep Learning de los últimos años se empieza a hacer notar, aunque de forma más lenta que en otros campos, en la medicina. Por primera vez, este tipo de técnicas salen del ámbito de la investigación y son utilizadas para el <strong>diagnóstico</strong>. Tradicionalmente los programas utilizados en diagnóstico eran <strong>sistemas expertos</strong>. Este tipo de programas simplemente se limitaban a pedir una serie de datos sobre el paciente, y obtenían conclusiones a partir de una serie de reglas que previamente habían tenido que ser definidas por especialistas. Sin embargo, con sistemas basados en Machine Learning, <strong>estas reglas son inferidas a partir de datos históricos</strong>. Una de las principales características del Machine Learning que le hace destacar sobre otros métodos tradicionales, es su capacidad de manejar enormes cantidades de predictores y encontrar complicados patrones entre ellos.</p>
<p>Además, debido a la gran cantidad de información no estructurada existente (imágenes, señales, textos, etc) en medicina, como era de esperar, el <strong>Deep Learning</strong> juego un papel esencial, permitiendo que los datos “hablen por sí mismos”.</p>
<h3 id="prognosis"><span class="header-section-number">3.5.1</span> Prognosis</h3>
<h3 id="diagnosis"><span class="header-section-number">3.5.2</span> Diagnosis</h3>
<h3 id="tratamiento"><span class="header-section-number">3.5.3</span> Tratamiento</h3>
<h3 id="clinician-workflow"><span class="header-section-number">3.5.4</span> Clinician workflow</h3>
<h3 id="correlación-no-implica-causalidad"><span class="header-section-number">3.5.5</span> Correlación no implica causalidad</h3>
<p>Aunque sea un mantra repetido hasta la saciedad en la literatura, esta advertencia merece una apartado propio en un trabajo de estas características, pues es algo a tener en cuenta y que implica tener mucha cautela al obtener conclusiones mediante este tipo de métodos.</p>
<p>En muchas ocasiones creemos, de forma errónea, que existe una relación de causa y efecto entre dos variables que están correlacionadas.</p>
<p>La correlación entre dos variables, puede ser debida a una tercera <strong>variable oculta</strong> que no tenemos por qué conocer o simplemente puede ser lo que conocemos como <strong>correlación espúrea</strong>, es decir, mera casualidad (que no causalidad).</p>
<p>Sin embargo, la falacia <strong>Cum hoc ergo propter hoc</strong> (en latín, “Con esto, por tanto a causa de esto”) sigue siendo estando muy presente en los medios de comunicación y en las <strong>pseudociencias</strong>.</p>
<p>Si alguna vez el lector divisa a un sujeto disfrazado de pirata, no lo tome por loco. Ese sujeto podría ser un seguidor de <strong>Bobby Henderson</strong>, creador de la iglesia pastafari que, cansado de argumentos de los creacionistas basados en esta falacia, realizó un estudio en el que se podía apreciar una clara correlación entre la temperatura global y el descenso del número de piratas. Es común, desde entonces, que los seguidores de Henderson se disfracen de piratas para recordarlo.</p>
<!-- TODO: Imagen de la grafica de piratas y temperatura
    https://jdcdn-wabisabiinvestme.netdna-ssl.com/wp-content/uploads/2016/06/1-1-768x483.jpg-->
<p>Otro ejemplo, muy popular, es la singular correlación entre el número de ahogados en piscinas en Estados Unidos y el número de apariciones en películas de Nicholas Cage.</p>
<!-- TODO: Imagen y explicación de
http://www.tylervigen.com/spurious-correlations-->
<p>Sin embargo, lejos de quedar en una mera anécdota como las anteriores, es extremadamente preocupante que existan familias en todo el mundo que estén decidiendo no vacunar a sus hijos debido a una aparente correlación en un estudio de 2010 entre el número de casos de autismo y las vacunaciones.</p>
<p>Por lo tanto, es necesaria una gran cautela antes de obtener conclusiones de los sistemas de Machine Learning. Además, no estaría de más, aunque no serán objeto de análisis en este trabajo, tener presentes el Sesgo del Superviviente y la Paradoja de Simpson).</p>
<h1 id="estado-del-arte"><span class="header-section-number">4</span> Estado del arte</h1>
<h2 id="detección-de-dr"><span class="header-section-number">4.1</span> Detección de DR</h2>
<h2 id="detección-de-dmae"><span class="header-section-number">4.2</span> Detección de DMAE</h2>
<h2 id="introduction"><span class="header-section-number">4.3</span> Introduction</h2>
<p>This is the introduction. Sed vulputate tortor at nisl blandit interdum. Cras sagittis massa ex, quis eleifend purus condimentum congue. Maecenas tristique, justo vitae efficitur mollis, mi nulla varius elit, in consequat ligula nulla ut augue. Phasellus diam sapien, placerat sit amet tempor non, lobortis tempus ante.</p>
<h2 id="method"><span class="header-section-number">4.4</span> Method</h2>
<p>Donec imperdiet, lectus vestibulum sagittis tempus, turpis dolor euismod justo, vel tempus neque libero sit amet tortor. Nam cursus commodo tincidunt.</p>
<h3 id="subsection-1"><span class="header-section-number">4.4.1</span> Subsection 1</h3>
<p>This is the first part of the methodology. Duis tempor sapien sed tellus ultrices blandit. Sed porta mauris tortor, eu vulputate arcu dapibus ac. Curabitur sodales at felis efficitur sollicitudin. Quisque at neque sollicitudin, mollis arcu vitae, faucibus tellus.</p>
<h3 id="subsection-2"><span class="header-section-number">4.4.2</span> Subsection 2</h3>
<p>This is the second part of the methodology. Sed ut ipsum ultrices, interdum ipsum vel, lobortis diam. Curabitur sit amet massa quis tortor molestie dapibus a at libero. Mauris mollis magna quis ante vulputate consequat. Integer leo turpis, suscipit ac venenatis pellentesque, efficitur non sem. Pellentesque eget vulputate turpis. Etiam id nibh at elit fermentum interdum.</p>
<!--
Comments can be added like this.
-->
<h2 id="results"><span class="header-section-number">4.5</span> Results</h2>
<p>These are the results. In vitae odio at libero elementum fermentum vel iaculis enim. Nullam finibus sapien in congue condimentum. Curabitur et ligula et ipsum mollis fringilla.</p>
<h2 id="discussion"><span class="header-section-number">4.6</span> Discussion</h2>
<p>Figure  shows how to add a figure. Donec ut lacinia nibh. Nam tincidunt augue et tristique cursus. Vestibulum sagittis odio nisl, a malesuada turpis blandit quis. Cras ultrices metus tempor laoreet sodales. Nam molestie ipsum ac imperdiet laoreet. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas.</p>
<!--
Figures can be added with the following syntax:
![my_caption \label{my_label}](source/figures/my_image.pdf){ width=50% }

For details on setting attributes like width and height, see:
http://pandoc.org/MANUAL.html#extension-link_attributes
-->
<figure>
<embed src="source/figures/example_figure.pdf" style="width:100.0%" /><figcaption>RV Calypso is a former British Royal Navy minesweeper converted into a research vessel for the oceanographic researcher Jacques-Yves Cousteau. It was equipped with a mobile laboratory for underwater field research. </figcaption>
</figure>
<h2 id="conclusion"><span class="header-section-number">4.7</span> Conclusion</h2>
<p>This is the conclusion to the chapter. Quisque nec purus a quam consectetur volutpat. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. In lorem justo, convallis quis lacinia eget, laoreet eu metus. Fusce blandit tellus tellus. Curabitur nec cursus odio. Quisque tristique eros nulla, vitae finibus lorem aliquam quis. Interdum et malesuada fames ac ante ipsum primis in faucibus.</p>
<h1 id="diseño-de-sistema-para-la-clasificación-automática-de-rd-y-dmae"><span class="header-section-number">5</span> Diseño de sistema para la clasificación automática de RD y DMAE</h1>
<h2 id="exploración-inicial-de-los-datos"><span class="header-section-number">5.1</span> Exploración inicial de los datos</h2>
<!-- ------------------------------------------------------------>
<h2 id="introduction-1"><span class="header-section-number">5.2</span> Introduction</h2>
<p>This is the introduction. Phasellus non purus id mauris aliquam rutrum vitae quis tellus. Maecenas rhoncus ligula nulla, fringilla placerat mi consectetur eu. Aenean nec metus ac est ornare posuere. Nunc ipsum lacus, gravida commodo turpis quis, rutrum eleifend erat. Pellentesque id lorem eget ante porta tincidunt nec nec tellus.</p>
<h2 id="method-1"><span class="header-section-number">5.3</span> Method</h2>
<p>Vivamus consectetur, velit in congue lobortis, massa massa lacinia urna, sollicitudin semper ipsum augue quis tortor. Donec quis nisl at arcu volutpat ultrices. Maecenas ex nibh, consequat ac blandit sit amet, molestie in odio. Morbi finibus libero et nisl dignissim, at ultricies ligula pulvinar.</p>
<h3 id="subsection-1-1"><span class="header-section-number">5.3.1</span> Subsection 1</h3>
<p>This is the first part of the methodology. Integer leo erat, commodo in lacus vel, egestas varius elit. Nulla eget magna quam. Nullam sollicitudin dolor ut ipsum varius tincidunt. Duis dignissim massa in ipsum accumsan imperdiet. Maecenas suscipit sapien sed dui pharetra blandit. Morbi fermentum est vel quam pretium maximus.</p>
<h3 id="subsection-2-1"><span class="header-section-number">5.3.2</span> Subsection 2</h3>
<p>This is the second part of the methodology. Nullam accumsan condimentum eros eu volutpat. Maecenas quis ligula tempor, interdum ante sit amet, aliquet sem. Fusce tellus massa, blandit id tempus at, cursus in tortor. Nunc nec volutpat ante. Phasellus dignissim ut lectus quis porta. Lorem ipsum dolor sit amet, consectetur adipiscing elit.</p>
<!--
Comments can be added like this.
-->
<h2 id="results-1"><span class="header-section-number">5.4</span> Results</h2>
<p>Table  shows us how to add a table. Integer tincidunt sed nisl eget pellentesque. Mauris eleifend, nisl non lobortis fringilla, sapien eros aliquet orci, vitae pretium massa neque eu turpis. Pellentesque tincidunt aliquet volutpat. Ut ornare dui id ex sodales laoreet.</p>
<!-- Force the table onto a newpage -->

<table style="width:89%;">
<caption>This is the table caption. Suspendisse blandit dolor sed tellus venenatis, venenatis fringilla turpis pretium. </caption>
<colgroup>
<col style="width: 27%" />
<col style="width: 33%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Column 1</th>
<th style="text-align: left;">Column 2</th>
<th style="text-align: left;">Column 3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Row 1</td>
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">0.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">Row 2</td>
<td style="text-align: left;">0.3</td>
<td style="text-align: left;">0.3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Row 3</td>
<td style="text-align: left;">0.4</td>
<td style="text-align: left;">0.4</td>
</tr>
<tr class="even">
<td style="text-align: left;">Row 4</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">0.6</td>
</tr>
</tbody>
</table>
<h2 id="discussion-1"><span class="header-section-number">5.5</span> Discussion</h2>
<p>This is the discussion. Etiam sit amet mi eros. Donec vel nisi sed purus gravida fermentum at quis odio. Vestibulum quis nisl sit amet justo maximus molestie. Maecenas vitae arcu erat. Nulla facilisi. Nam pretium mauris eu enim porttitor, a mattis velit dictum. Nulla sit amet ligula non mauris volutpat fermentum quis vitae sapien.</p>
<h2 id="conclusion-1"><span class="header-section-number">5.6</span> Conclusion</h2>
<p>This is the conclusion to the chapter. Nullam porta tortor id vehicula interdum. Quisque pharetra, neque ut accumsan suscipit, orci orci commodo tortor, ac finibus est turpis eget justo. Cras sodales nibh nec mauris laoreet iaculis. Morbi volutpat orci felis, id condimentum nulla suscipit eu. Fusce in turpis quis ligula tempus scelerisque eget quis odio. Vestibulum et dolor id erat lobortis ullamcorper quis at sem.</p>
<h1 id="resultados-obtenidos"><span class="header-section-number">6</span> Resultados obtenidos</h1>
<h2 id="introduction-2"><span class="header-section-number">6.1</span> Introduction</h2>
<p>This is the introduction. Nunc lorem odio, laoreet eu turpis at, condimentum sagittis diam. Phasellus metus ligula, auctor ac nunc vel, molestie mattis libero. Praesent id posuere ex, vel efficitur nibh. Quisque vestibulum accumsan lacus vitae mattis.</p>
<h2 id="method-2"><span class="header-section-number">6.2</span> Method</h2>
<p>In tincidunt viverra dolor, ac pharetra tellus faucibus eget. Pellentesque tempor a enim nec venenatis. Morbi blandit magna imperdiet posuere auctor. Maecenas in maximus est.</p>
<h3 id="subsection-1-2"><span class="header-section-number">6.2.1</span> Subsection 1</h3>
<p>This is the first part of the methodology. Praesent mollis sem diam, sit amet tristique lacus vulputate quis. Vivamus rhoncus est rhoncus tellus lacinia, a interdum sem egestas. Curabitur quis urna vel quam blandit semper vitae a leo. Nam vel lectus lectus.</p>
<h3 id="subsection-2-2"><span class="header-section-number">6.2.2</span> Subsection 2</h3>
<p>This is the second part of the methodology. Aenean vel pretium tortor. Aliquam erat volutpat. Quisque quis lobortis mi. Nulla turpis leo, ultrices nec nulla non, ullamcorper laoreet risus.</p>
<!--
Comments can be added like this.
-->
<h2 id="results-2"><span class="header-section-number">6.3</span> Results</h2>
<p>These are the results. Curabitur vulputate nisl non ante tincidunt tempor. Aenean porta nisi quam, sed ornare urna congue sed. Curabitur in sapien justo. Quisque pulvinar ullamcorper metus, eu varius mauris pellentesque et. In hac habitasse platea dictumst. Pellentesque nec porttitor libero. Duis et magna a massa lacinia cursus.</p>
<h2 id="discussion-2"><span class="header-section-number">6.4</span> Discussion</h2>
<p>This is the discussion. Curabitur gravida nisl id gravida congue. Duis est nisi, sagittis eget accumsan ullamcorper, semper quis turpis. Mauris ultricies diam metus, sollicitudin ultricies turpis lobortis vitae. Ut egestas vehicula enim, porta molestie neque consectetur placerat. Integer iaculis sapien dolor, non porta nibh condimentum ut.</p>
<h2 id="conclusion-2"><span class="header-section-number">6.5</span> Conclusion</h2>
<p>This is the conclusion to the chapter. Nulla sed condimentum lectus. Duis sed tempor erat, at cursus lacus. Nam vitae tempus arcu, id vestibulum sapien. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.</p>
<h1 id="conclusiones"><span class="header-section-number">7</span> Conclusiones</h1>
<!--
A chapter that concludes the thesis by summarising the learning points
and outlining future areas for research
-->
<h2 id="thesis-summary"><span class="header-section-number">7.1</span> Thesis summary</h2>
<p>In summary, pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Nunc eleifend, ex a luctus porttitor, felis ex suscipit tellus, ut sollicitudin sapien purus in libero. Nulla blandit eget urna vel tempus. Praesent fringilla dui sapien, sit amet egestas leo sollicitudin at.</p>
<h2 id="future-work"><span class="header-section-number">7.2</span> Future work</h2>
<p>There are several potential directions for extending this thesis. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam gravida ipsum at tempor tincidunt. Aliquam ligula nisl, blandit et dui eu, eleifend tempus nibh. Nullam eleifend sapien eget ante hendrerit commodo. Pellentesque pharetra erat sit amet dapibus scelerisque.</p>
<p>Vestibulum suscipit tellus risus, faucibus vulputate orci lobortis eget. Nunc varius sem nisi. Nunc tempor magna sapien, euismod blandit elit pharetra sed. In dapibus magna convallis lectus sodales, a consequat sem euismod. Curabitur in interdum purus. Integer ultrices laoreet aliquet. Nulla vel dapibus urna. Nunc efficitur erat ac nisi auctor sodales.</p>
<h1 id="appendix-1-some-extra-stuff" class="unnumbered">Appendix 1: Some extra stuff</h1>
<!-- 
This could be a list of papers by the author for example 
-->
<p>Add appendix 1 here. Vivamus hendrerit rhoncus interdum. Sed ullamcorper et augue at porta. Suspendisse facilisis imperdiet urna, eu pellentesque purus suscipit in. Integer dignissim mattis ex aliquam blandit. Curabitur lobortis quam varius turpis ultrices egestas.</p>
<h1 id="appendix-2-some-more-extra-stuff" class="unnumbered">Appendix 2: Some more extra stuff</h1>
<!-- 
This could include extra figures or raw data
-->
<p>Add appendix 2 here. Aliquam rhoncus mauris ac neque imperdiet, in mattis eros aliquam. Etiam sed massa et risus posuere rutrum vel et mauris. Integer id mauris sed arcu venenatis finibus. Etiam nec hendrerit purus, sed cursus nunc. Pellentesque ac luctus magna. Aenean non posuere enim, nec hendrerit lacus. Etiam lacinia facilisis tempor. Aenean dictum nunc id felis rhoncus aliquam.</p>

<!-- 
Do not edit this page.

References are automatically generated from the BibTex file (References.bib)

...which you should create using your reference manager.
-->
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-idf2017">
<p>Anon, IDF diabetes atlas 8th edition 2017.</p>
</div>
<div id="ref-IAPB">
<p>Anon, International agency for the prevention of blindness (iapb). Diabetic retinopathy.</p>
</div>
<div id="ref-bjorvig2002economic">
<p>Bjørvig, S., Johansen, M.A. &amp; Fossen, K., 2002. An economic analysis of screening for diabetic retinopathy. <em>Journal of Telemedicine and Telecare</em>, 8(1), pp.32–35.</p>
</div>
<div id="ref-darwin2004origin">
<p>Darwin, C., 2004. <em>On the origin of species, 1859</em>, Routledge.</p>
</div>
<div id="ref-guariguata2014global">
<p>Guariguata, L. et al., 2014. Global estimates of diabetes prevalence for 2013 and projections for 2035. <em>Diabetes research and clinical practice</em>, 103(2), pp.137–149.</p>
</div>
<div id="ref-oyster1999human">
<p>Oyster, C.W., 1999. The human eye. <em>Sunderland, MA: Sinauer</em>.</p>
</div>
<div id="ref-pan2009survey">
<p>Pan, S.J. &amp; Yang, Q., 2009. A survey on transfer learning. <em>IEEE Transactions on knowledge and data engineering</em>, 22(10), pp.1345–1359.</p>
</div>
<div id="ref-pascolini2012global">
<p>Pascolini, D. &amp; Mariotti, S.P., 2012. Global estimates of visual impairment: 2010. <em>British Journal of Ophthalmology</em>, 96(5), pp.614–618.</p>
</div>
<div id="ref-rajkomar2019machine">
<p>Rajkomar, A., Dean, J. &amp; Kohane, I., 2019. Machine learning in medicine. <em>New England Journal of Medicine</em>, 380(14), pp.1347–1358.</p>
</div>
<div id="ref-ruamviboonsuk2005screening">
<p>Ruamviboonsuk, P. et al., 2005. Screening for diabetic retinopathy in rural area using single-field, digital fundus images. <em>J Med Assoc Thai</em>, 88(2), pp.176–180.</p>
</div>
<div id="ref-schwartz1970medicine">
<p>Schwartz, W.B., 1970. Medicine and the computer: The promise and problems of change. In <em>Use and impact of computers in clinical medicine</em>. Springer, pp. 321–335.</p>
</div>
<div id="ref-schwartz1986artificial">
<p>Schwartz, W.B., Patil, R.S. &amp; Szolovits, P., 1986. Artificial intelligence in medicine where do we stand. <em>Jurimetrics J.</em>, 27, p.362.</p>
</div>
<div id="ref-sellahewa2014grader">
<p>Sellahewa, L. et al., 2014. Grader agreement, and sensitivity and specificity of digital photography in a community optometry-based diabetic eye screening program. <em>Clinical Ophthalmology (Auckland, NZ)</em>, 8, p.1345.</p>
</div>
<div id="ref-wong2014global">
<p>Wong, W.L. et al., 2014. Global prevalence of age-related macular degeneration and disease burden projection for 2020 and 2040: A systematic review and meta-analysis. <em>The Lancet Global Health</em>, 2(2), pp.e106–e116.</p>
</div>
<div id="ref-zhang2017direct">
<p>Zhang, X. et al., 2017. Direct medical cost associated with diabetic retinopathy severity in type 2 diabetes in singapore. <em>PloS one</em>, 12(7), p.e0180949.</p>
</div>
<div id="ref-zheng2012worldwide">
<p>Zheng, Y., He, M. &amp; Congdon, N., 2012. The worldwide epidemic of diabetic retinopathy. <em>Indian journal of ophthalmology</em>, 60(5), p.428.</p>
</div>
<div id="ref-zhu2001eye">
<p>Zhu, J., Zhang, E. &amp; Del Rio-Tsonis, K., 2001. Eye anatomy. <em>e LS</em>.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Basada en la clasificación de https://idrid.grand-challenge.org/grading/<a href="#fnref1" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn2" role="doc-endnote"><p>Artificial General Intelligence: Aquella inteligencia artificial que puede realizar con éxito cualquier tarea intelectual de cualquier ser humano<a href="#fnref2" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn3" role="doc-endnote"><p>http://cs231n.github.io/transfer-learning/<a href="#fnref3" class="footnote-back" role="doc-backlink">↩</a></p></li>
</ol>
</section>
            </body>
</html>

